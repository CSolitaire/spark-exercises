{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark 101 Exercises\n",
    "### Corey Solitaire\n",
    "`11.24.2020`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pydataset import data\n",
    "from vega_datasets import data\n",
    "\n",
    "from pyspark.sql.functions import col, expr, concat, sum, avg, min, max, count, mean, round\n",
    "from pyspark.sql.functions import lit, regexp_extract, regexp_replace, when,asc, desc, month, year, quarter  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Method to create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Reference Material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pandas</th>\n",
       "      <th>spark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>csv</th>\n",
       "      <td>pd.read_csv(\"myfile.csv\")</td>\n",
       "      <td>spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>json</th>\n",
       "      <td>pd.read_json(\"myfile.json\")</td>\n",
       "      <td>spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st n rows</th>\n",
       "      <td>pd_df.head()</td>\n",
       "      <td>sp_df.show(), .head(), .take()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st row</th>\n",
       "      <td>pd_df.head(1)</td>\n",
       "      <td>sp_df.first()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary statistics</th>\n",
       "      <td>pd_df.describe()</td>\n",
       "      <td>sp_df.describe()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column names</th>\n",
       "      <td>pd_df.columns</td>\n",
       "      <td>sp_df.columns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># rows</th>\n",
       "      <td>len(pd_df)</td>\n",
       "      <td>sp_df.count()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># distinct rows</th>\n",
       "      <td>len(pd_df.drop_duplicates())</td>\n",
       "      <td>sp_df.distinct().count()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>df schema info</th>\n",
       "      <td>pd_df.info()</td>\n",
       "      <td>sp_df.printSchema()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select columns</th>\n",
       "      <td>pd_df[[\"col1\", \"col2\"]]</td>\n",
       "      <td>sp_df.select(sp_df.col1, sp_df.col2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conditional filtering</th>\n",
       "      <td>pd_df[pd_df.c1 &gt; 0]</td>\n",
       "      <td>sp_df.filter(df.c1 &gt; 0), sp_df.where(df.c1 &gt; 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conditional assigning</th>\n",
       "      <td>np.where(pd_df.c1.array &gt; 0, \"positive\")</td>\n",
       "      <td>sp_df.select(df.c1, when(df.c1 &gt; 0, \"positive\").alias(\"number_sign\"))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conditional assigning with else</th>\n",
       "      <td>np.where(pd_df.c1.array &gt; 0, \"pos\", \"neg\")</td>\n",
       "      <td>sp_df.select(df.c1, when(df.c1 &gt; 0, \"pos\").otherwise(\"neg\").alias(\"number_sign\"))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort 1 col asc</th>\n",
       "      <td>pd_df.sort_values(by=[\"c1\"])</td>\n",
       "      <td>sp_df.sort(sp_df.c1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort 2+ cols asc</th>\n",
       "      <td>pd_df.sort_values(by=[\"c1\",\"c2\"])</td>\n",
       "      <td>sp_df.sort(sp_df.c1, sp_df.c2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort 2+ cols desc/asc</th>\n",
       "      <td>pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=[False, True])</td>\n",
       "      <td>sp_df.sort(sp_df.c1.desc(), sp_df.c2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort 2+ cols desc</th>\n",
       "      <td>pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=False)</td>\n",
       "      <td>sp_df.sort(desc(\"c1\"), desc(\"c2\")) OR sp_df.sort(col(\"c1\").desc(), col(\"c2\").desc())</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     pandas  \\\n",
       "csv                                                               pd.read_csv(\"myfile.csv\")   \n",
       "json                                                            pd.read_json(\"myfile.json\")   \n",
       "1st n rows                                                                     pd_df.head()   \n",
       "1st row                                                                       pd_df.head(1)   \n",
       "summary statistics                                                         pd_df.describe()   \n",
       "column names                                                                  pd_df.columns   \n",
       "# rows                                                                           len(pd_df)   \n",
       "# distinct rows                                                len(pd_df.drop_duplicates())   \n",
       "df schema info                                                                 pd_df.info()   \n",
       "select columns                                                      pd_df[[\"col1\", \"col2\"]]   \n",
       "conditional filtering                                                   pd_df[pd_df.c1 > 0]   \n",
       "conditional assigning                              np.where(pd_df.c1.array > 0, \"positive\")   \n",
       "conditional assigning with else                  np.where(pd_df.c1.array > 0, \"pos\", \"neg\")   \n",
       "sort 1 col asc                                                 pd_df.sort_values(by=[\"c1\"])   \n",
       "sort 2+ cols asc                                          pd_df.sort_values(by=[\"c1\",\"c2\"])   \n",
       "sort 2+ cols desc/asc            pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=[False, True])   \n",
       "sort 2+ cols desc                        pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=False)   \n",
       "\n",
       "                                                                                                                spark  \n",
       "csv                                                          spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")  \n",
       "json                                spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")  \n",
       "1st n rows                                                                             sp_df.show(), .head(), .take()  \n",
       "1st row                                                                                                 sp_df.first()  \n",
       "summary statistics                                                                                   sp_df.describe()  \n",
       "column names                                                                                            sp_df.columns  \n",
       "# rows                                                                                                  sp_df.count()  \n",
       "# distinct rows                                                                              sp_df.distinct().count()  \n",
       "df schema info                                                                                    sp_df.printSchema()  \n",
       "select columns                                                                   sp_df.select(sp_df.col1, sp_df.col2)  \n",
       "conditional filtering                                                 sp_df.filter(df.c1 > 0), sp_df.where(df.c1 > 0)  \n",
       "conditional assigning                           sp_df.select(df.c1, when(df.c1 > 0, \"positive\").alias(\"number_sign\"))  \n",
       "conditional assigning with else     sp_df.select(df.c1, when(df.c1 > 0, \"pos\").otherwise(\"neg\").alias(\"number_sign\"))  \n",
       "sort 1 col asc                                                                                   sp_df.sort(sp_df.c1)  \n",
       "sort 2+ cols asc                                                                       sp_df.sort(sp_df.c1, sp_df.c2)  \n",
       "sort 2+ cols desc/asc                                                           sp_df.sort(sp_df.c1.desc(), sp_df.c2)  \n",
       "sort 2+ cols desc                sp_df.sort(desc(\"c1\"), desc(\"c2\")) OR sp_df.sort(col(\"c1\").desc(), col(\"c2\").desc())  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_v_spark = pd.DataFrame([['pd.read_csv(\"myfile.csv\")', \n",
    "                            'spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")'], \n",
    "                           ['pd.read_json(\"myfile.json\")', \n",
    "                            'spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")']], \n",
    "                          index = ['csv', 'json'], \n",
    "                          columns = ['pandas', 'spark'])\n",
    "\n",
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['pd_df.head()', 'sp_df.show(), .head(), .take()'],\n",
    "                                             ['pd_df.head(1)', 'sp_df.first()'],\n",
    "                                             ['pd_df.describe()', 'sp_df.describe()'],\n",
    "                                             ['pd_df.columns', 'sp_df.columns'],\n",
    "                                             ['len(pd_df)', 'sp_df.count()'],\n",
    "                                             ['len(pd_df.drop_duplicates())', 'sp_df.distinct().count()'],\n",
    "                                             ['pd_df.info()', 'sp_df.printSchema()']\n",
    "                                            ],\n",
    "                                            index = ['1st n rows', '1st row','summary statistics', \n",
    "                                                     'column names', '# rows', '# distinct rows', \n",
    "                                                     'df schema info'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "\n",
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['pd_df[[\"col1\", \"col2\"]]', \n",
    "                                              'sp_df.select(sp_df.col1, sp_df.col2)']\n",
    "                                            ],\n",
    "                                            index = ['select columns'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "\n",
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['pd_df[pd_df.c1 > 0]', 'sp_df.filter(df.c1 > 0), sp_df.where(df.c1 > 0)'],\n",
    "                                            ],\n",
    "                                            index = ['conditional filtering'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "\n",
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['np.where(pd_df.c1.array > 0, \"positive\")', \n",
    "                                              'sp_df.select(df.c1, when(df.c1 > 0, \"positive\").alias(\"number_sign\"))'],\n",
    "                                            ],\n",
    "                                            index = ['conditional assigning'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "\n",
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['np.where(pd_df.c1.array > 0, \"pos\", \"neg\")', \n",
    "                                              'sp_df.select(df.c1, when(df.c1 > 0, \"pos\").otherwise(\"neg\").alias(\"number_sign\"))'],\n",
    "                                            ],\n",
    "                                            index = ['conditional assigning with else'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "\n",
    "\n",
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['pd_df.sort_values(by=[\"c1\"])', \n",
    "                                              'sp_df.sort(sp_df.c1)'],\n",
    "                                             ['pd_df.sort_values(by=[\"c1\",\"c2\"])',\n",
    "                                              'sp_df.sort(sp_df.c1, sp_df.c2)'],\n",
    "                                             ['pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=[False, True])',\n",
    "                                              'sp_df.sort(sp_df.c1.desc(), sp_df.c2)'],\n",
    "                                             ['pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=False)', \n",
    "                                              'sp_df.sort(desc(\"c1\"), desc(\"c2\")) OR sp_df.sort(col(\"c1\").desc(), col(\"c2\").desc())']\n",
    "                                            ],\n",
    "                                            index = ['sort 1 col asc', 'sort 2+ cols asc', 'sort 2+ cols desc/asc', 'sort 2+ cols desc'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "pd_v_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1. Create a spark data frame that contains your favorite programming languages.\n",
    "\n",
    "### -The name of the column should be language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>python</th>\n",
       "      <th>javascript</th>\n",
       "      <th>r</th>\n",
       "      <th>java</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>csv</th>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>json</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     python javascript    r java\n",
       "csv     yes         no  yes   no\n",
       "json    yes        yes  yes  yes"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Dataframe\n",
    "lang_df = pd.DataFrame([['yes', 'no', 'yes', 'no'], \n",
    "                        ['yes', 'yes', 'yes', 'yes']], \n",
    "                        index = ['csv', 'json'], \n",
    "                        columns = ['python', 'javascript', 'r', 'java'])\n",
    "\n",
    "# to display and see all text in dataframe\n",
    "pd.set_option('display.max_colwidth', 10000)\n",
    "\n",
    "\n",
    "lang_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+----+\n",
      "|python|javascript|  r|java|\n",
      "+------+----------+---+----+\n",
      "|   yes|        no|yes|  no|\n",
      "|   yes|       yes|yes| yes|\n",
      "+------+----------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to spark\n",
    "sp_df = spark.createDataFrame(lang_df)\n",
    "sp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -View the schema of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- python: string (nullable = true)\n",
      " |-- javascript: string (nullable = true)\n",
      " |-- r: string (nullable = true)\n",
      " |-- java: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -Output the shape of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4)\n"
     ]
    }
   ],
   "source": [
    "print((sp_df.count(), len(sp_df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -Show the first 5 records in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+----+\n",
      "|python|javascript|  r|java|\n",
      "+------+----------+---+----+\n",
      "|   yes|        no|yes|  no|\n",
      "|   yes|       yes|yes| yes|\n",
      "+------+----------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2. Load the mpg dataset as a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|manufacturer|model|displ|year|cyl|     trans|drv|cty|hwy| fl|  class|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|        audi|   a4|  1.8|1999|  4|  auto(l5)|  f| 18| 29|  p|compact|\n",
      "|        audi|   a4|  1.8|1999|  4|manual(m5)|  f| 21| 29|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|manual(m6)|  f| 20| 31|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|  auto(av)|  f| 21| 30|  p|compact|\n",
      "|        audi|   a4|  2.8|1999|  6|  auto(l5)|  f| 16| 26|  p|compact|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg = spark.createDataFrame(data(\"mpg\"))\n",
    "mpg.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - A. Create 1 column of output that contains a message like the one below:\n",
    "   `The 1999 audi a4 has a 4 cylinder engine.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------+\n",
      "|Discription                                                  |\n",
      "+-------------------------------------------------------------+\n",
      "|The 1999 audi a4 has a 4 cylinder engine                     |\n",
      "|The 1999 audi a4 has a 4 cylinder engine                     |\n",
      "|The 2008 audi a4 has a 4 cylinder engine                     |\n",
      "|The 2008 audi a4 has a 4 cylinder engine                     |\n",
      "|The 1999 audi a4 has a 6 cylinder engine                     |\n",
      "|The 1999 audi a4 has a 6 cylinder engine                     |\n",
      "|The 2008 audi a4 has a 6 cylinder engine                     |\n",
      "|The 1999 audi a4 quattro has a 4 cylinder engine             |\n",
      "|The 1999 audi a4 quattro has a 4 cylinder engine             |\n",
      "|The 2008 audi a4 quattro has a 4 cylinder engine             |\n",
      "|The 2008 audi a4 quattro has a 4 cylinder engine             |\n",
      "|The 1999 audi a4 quattro has a 6 cylinder engine             |\n",
      "|The 1999 audi a4 quattro has a 6 cylinder engine             |\n",
      "|The 2008 audi a4 quattro has a 6 cylinder engine             |\n",
      "|The 2008 audi a4 quattro has a 6 cylinder engine             |\n",
      "|The 1999 audi a6 quattro has a 6 cylinder engine             |\n",
      "|The 2008 audi a6 quattro has a 6 cylinder engine             |\n",
      "|The 2008 audi a6 quattro has a 8 cylinder engine             |\n",
      "|The 2008 chevrolet c1500 suburban 2wd has a 8 cylinder engine|\n",
      "|The 2008 chevrolet c1500 suburban 2wd has a 8 cylinder engine|\n",
      "+-------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(concat(lit('The '), mpg.year, lit(' '), mpg['manufacturer'], lit(' '), mpg.model, lit(' has a '), mpg.cyl, lit(\" cylinder\"), lit(' engine')).alias(\"Discription\")).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - B. For each vehicle.\n",
    "        - Transform the trans column so that it only contains either manual or auto.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|transonly|\n",
      "+---------+\n",
      "|     auto|\n",
      "|   manual|\n",
      "|   manual|\n",
      "|     auto|\n",
      "|     auto|\n",
      "|   manual|\n",
      "|     auto|\n",
      "|   manual|\n",
      "|     auto|\n",
      "|   manual|\n",
      "|     auto|\n",
      "|     auto|\n",
      "|   manual|\n",
      "|     auto|\n",
      "|   manual|\n",
      "|     auto|\n",
      "|     auto|\n",
      "|     auto|\n",
      "|     auto|\n",
      "|     auto|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#regex \n",
    "mpg.select(regexp_replace(\"trans\", r\"\\([^)]*\\)\", \"\").alias(\"transonly\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----+---+------+---+---+---+---+-------+\n",
      "|manufacturer|model|displ|year|cyl| trans|drv|cty|hwy| fl|  class|\n",
      "+------------+-----+-----+----+---+------+---+---+---+---+-------+\n",
      "|        audi|   a4|  1.8|1999|  4|  auto|  f| 18| 29|  p|compact|\n",
      "|        audi|   a4|  1.8|1999|  4|manual|  f| 21| 29|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|manual|  f| 20| 31|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|  auto|  f| 21| 30|  p|compact|\n",
      "|        audi|   a4|  2.8|1999|  6|  auto|  f| 16| 26|  p|compact|\n",
      "|        audi|   a4|  2.8|1999|  6|manual|  f| 18| 26|  p|compact|\n",
      "+------------+-----+-----+----+---+------+---+---+---+---+-------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# not regex\n",
    "mpg = mpg.withColumn(\"trans\", when(mpg.trans.startswith(\"a\"), \"auto\").otherwise(\"manual\")).show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3. Load the tips dataset as a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips = spark.createDataFrame(data(\"tips\"))\n",
    "tips.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - A. What percentage of observations are smokers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|smoker|count(smoker)|\n",
      "+------+-------------+\n",
      "|    No|          151|\n",
      "|   Yes|           93|\n",
      "+------+-------------+\n",
      "\n",
      "+------+-------------------------------+\n",
      "|smoker|round((count(smoker) / 244), 2)|\n",
      "+------+-------------------------------+\n",
      "|    No|                           0.62|\n",
      "|   Yes|                           0.38|\n",
      "+------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips.groupby(tips.smoker).agg(count(tips.smoker)).show()\n",
    "\n",
    "\n",
    "tips.groupBy('smoker').agg(round(count(tips.smoker)/ tips.count(),2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - B. Create a column that contains the tip percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "| (tip / total_bill)|\n",
      "+-------------------+\n",
      "|0.05944673337257211|\n",
      "|0.16054158607350097|\n",
      "|0.16658733936220846|\n",
      "| 0.1397804054054054|\n",
      "|0.14680764538430255|\n",
      "|0.18623962040332148|\n",
      "|0.22805017103762829|\n",
      "|0.11607142857142858|\n",
      "|0.13031914893617022|\n",
      "| 0.2185385656292287|\n",
      "| 0.1665043816942551|\n",
      "|0.14180374361883155|\n",
      "|0.10181582360570687|\n",
      "|0.16277807921866522|\n",
      "|0.20364126770060686|\n",
      "|0.18164967562557924|\n",
      "| 0.1616650532429816|\n",
      "|0.22774708410067526|\n",
      "|0.20624631703005306|\n",
      "|0.16222760290556903|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips.select(tips.tip / tips.total_bill).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+--------------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|tip_percentage|\n",
      "+----------+----+------+------+---+------+----+--------------+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|           6.0|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|          16.0|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|          17.0|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|          14.0|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|          15.0|\n",
      "|     25.29|4.71|  Male|    No|Sun|Dinner|   4|          19.0|\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|          23.0|\n",
      "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|          12.0|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|          13.0|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|          22.0|\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|          17.0|\n",
      "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|          14.0|\n",
      "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|          10.0|\n",
      "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|          16.0|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|          20.0|\n",
      "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|          18.0|\n",
      "|     10.33|1.67|Female|    No|Sun|Dinner|   3|          16.0|\n",
      "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|          23.0|\n",
      "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|          21.0|\n",
      "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|          16.0|\n",
      "+----------+----+------+------+---+------+----+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips = tips.withColumn('tip_percentage', expr('Round((tip/total_bill) * 100)'))\n",
    "tips.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - C. Calculate the average tip percentage for each combination of sex and smoker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----------------------------------------+\n",
      "|smoker|   sex|round(avg(((tip / total_bill) * 100)), 2)|\n",
      "+------+------+-----------------------------------------+\n",
      "|    No|Female|                                    15.69|\n",
      "|    No|  Male|                                    16.07|\n",
      "|   Yes|  Male|                                    15.28|\n",
      "|   Yes|Female|                                    18.22|\n",
      "+------+------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips.groupBy(tips.smoker, tips.sex).agg(round(avg(tips.tip/tips.total_bill * 100), 2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4. Use the seattle weather dataset referenced in the lesson to answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------+--------+----+-------+\n",
      "|      date|precipitation|temp_max|temp_min|wind|weather|\n",
      "+----------+-------------+--------+--------+----+-------+\n",
      "|2012-01-01|          0.0|    12.8|     5.0| 4.7|drizzle|\n",
      "|2012-01-02|         10.9|    10.6|     2.8| 4.5|   rain|\n",
      "|2012-01-03|          0.8|    11.7|     7.2| 2.3|   rain|\n",
      "|2012-01-04|         20.3|    12.2|     5.6| 4.7|   rain|\n",
      "|2012-01-05|          1.3|     8.9|     2.8| 6.1|   rain|\n",
      "|2012-01-06|          2.5|     4.4|     2.2| 2.2|   rain|\n",
      "+----------+-------------+--------+--------+----+-------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vega_datasets import data\n",
    "weather = data.seattle_weather().assign(date=lambda df: df.date.astype(str))\n",
    "weather = spark.createDataFrame(weather)\n",
    "weather.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -Convert the temperatures to farenheight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------+--------+----+-------+\n",
      "|      date|precipitation|temp_max|temp_min|wind|weather|\n",
      "+----------+-------------+--------+--------+----+-------+\n",
      "|2012-01-01|          0.0|   55.04|     5.0| 4.7|drizzle|\n",
      "|2012-01-02|         10.9|   51.08|     2.8| 4.5|   rain|\n",
      "|2012-01-03|          0.8|   53.06|     7.2| 2.3|   rain|\n",
      "|2012-01-04|         20.3|   53.96|     5.6| 4.7|   rain|\n",
      "|2012-01-05|          1.3|   48.02|     2.8| 6.1|   rain|\n",
      "|2012-01-06|          2.5|   39.92|     2.2| 2.2|   rain|\n",
      "|2012-01-07|          0.0|   44.96|     2.8| 2.3|   rain|\n",
      "|2012-01-08|          0.0|    50.0|     2.8| 2.0|    sun|\n",
      "|2012-01-09|          4.3|   48.92|     5.0| 3.4|   rain|\n",
      "|2012-01-10|          1.0|   42.98|     0.6| 3.4|   rain|\n",
      "|2012-01-11|          0.0|   42.98|    -1.1| 5.1|    sun|\n",
      "|2012-01-12|          0.0|   42.98|    -1.7| 1.9|    sun|\n",
      "|2012-01-13|          0.0|    41.0|    -2.8| 1.3|    sun|\n",
      "|2012-01-14|          4.1|   39.92|     0.6| 5.3|   snow|\n",
      "|2012-01-15|          5.3|   33.98|    -3.3| 3.2|   snow|\n",
      "|2012-01-16|          2.5|   35.06|    -2.8| 5.0|   snow|\n",
      "|2012-01-17|          8.1|   37.94|     0.0| 5.6|   snow|\n",
      "|2012-01-18|         19.8|    32.0|    -2.8| 5.0|   snow|\n",
      "|2012-01-19|         15.2|   30.02|    -2.8| 1.6|   snow|\n",
      "|2012-01-20|         13.5|   44.96|    -1.1| 2.3|   snow|\n",
      "+----------+-------------+--------+--------+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather = weather.withColumn(\"temp_max\", round(weather.temp_max*9/5 + 32, 2))\n",
    "weater = weather.withColumn(\"temp_min\", round(weather.temp_min*9/5 + 32, 2))\n",
    "weather.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -Which month has the most rain, on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|month|    total_rainfall|\n",
      "+-----+------------------+\n",
      "|    1|465.99999999999994|\n",
      "|    2|             422.0|\n",
      "|    3|             606.2|\n",
      "|    4|             375.4|\n",
      "|    5|             207.5|\n",
      "|    6|             132.9|\n",
      "|    7|              48.2|\n",
      "|    8|             163.7|\n",
      "|    9|235.49999999999997|\n",
      "|   10|             503.4|\n",
      "|   11|             642.5|\n",
      "|   12| 622.7000000000002|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    weather.withColumn(\"month\", month(\"date\"))\n",
    "    .groupBy(\"month\")\n",
    "    .agg(sum(\"precipitation\").alias(\"total_rainfall\"))\n",
    "    .sort(\"month\")\n",
    "    .show()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -Which year was the windiest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|year|        total_wind|\n",
      "+----+------------------+\n",
      "|   1|             389.2|\n",
      "|   2|             427.9|\n",
      "|   3|443.90000000000003|\n",
      "|   4|             422.9|\n",
      "|   5|             386.9|\n",
      "|   6| 375.7000000000001|\n",
      "|   7|             361.0|\n",
      "|   8|             341.1|\n",
      "|   9|             355.6|\n",
      "|  10|             364.5|\n",
      "|  11|             417.9|\n",
      "|  12|448.69999999999993|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    weather.withColumn(\"year\", month(\"date\"))\n",
    "    .groupBy(\"year\")\n",
    "    .agg(sum(\"wind\").alias(\"total_wind\"))\n",
    "    .sort(\"year\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -What is the most frequent type of weather in January?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|weather|count(weather)|\n",
      "+-------+--------------+\n",
      "|    fog|            38|\n",
      "|drizzle|            10|\n",
      "|   rain|            35|\n",
      "|    sun|            33|\n",
      "|   snow|             8|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    weather.filter(month(\"date\") == 1)\n",
    "    .groupBy(\"weather\")\n",
    "    .agg(count(\"weather\"))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -What is the average high and low tempurature on sunny days in July in 2013 and 2014?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|     avg(temp_min)|\n",
      "+------------------+\n",
      "|13.183333333333335|\n",
      "+------------------+\n",
      "\n",
      "+-----------------+\n",
      "|    avg(temp_max)|\n",
      "+-----------------+\n",
      "|79.85333333333334|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    weather.filter(month(\"date\") == 7)\n",
    "    .filter(year(\"date\") == 2013 & 2014)\n",
    "    .filter(weather.weather == 'sun')\n",
    "    .agg(mean(\"temp_min\"))\n",
    "    .show()\n",
    ")\n",
    "\n",
    "(\n",
    "    weather.filter(month(\"date\") == 7)\n",
    "    .filter(year(\"date\") == 2013 % 2014)\n",
    "    .filter(weather.weather == 'sun')\n",
    "    .agg(mean(\"temp_max\"))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -What percentage of days were rainy in q3 of 2015?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -For each year, find what percentage of days it rained (had non-zero precipitation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
