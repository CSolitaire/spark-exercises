{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark 101 Exercises\n",
    "### Corey Solitaire\n",
    "`11.24.2020`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pydataset import data\n",
    "from vega_datasets import data\n",
    "\n",
    "from pyspark.sql.functions import col, expr, concat, sum, avg, min, max, count, mean, round\n",
    "from pyspark.sql.functions import lit, regexp_extract, regexp_replace, when,asc, desc, month, year, quarter  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Method to create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Reference Material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pandas</th>\n",
       "      <th>spark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>csv</th>\n",
       "      <td>pd.read_csv(\"myfile.csv\")</td>\n",
       "      <td>spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>json</th>\n",
       "      <td>pd.read_json(\"myfile.json\")</td>\n",
       "      <td>spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st n rows</th>\n",
       "      <td>pd_df.head()</td>\n",
       "      <td>sp_df.show(), .head(), .take()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st row</th>\n",
       "      <td>pd_df.head(1)</td>\n",
       "      <td>sp_df.first()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary statistics</th>\n",
       "      <td>pd_df.describe()</td>\n",
       "      <td>sp_df.describe()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column names</th>\n",
       "      <td>pd_df.columns</td>\n",
       "      <td>sp_df.columns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># rows</th>\n",
       "      <td>len(pd_df)</td>\n",
       "      <td>sp_df.count()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># distinct rows</th>\n",
       "      <td>len(pd_df.drop_duplicates())</td>\n",
       "      <td>sp_df.distinct().count()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>df schema info</th>\n",
       "      <td>pd_df.info()</td>\n",
       "      <td>sp_df.printSchema()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select columns</th>\n",
       "      <td>pd_df[[\"col1\", \"col2\"]]</td>\n",
       "      <td>sp_df.select(sp_df.col1, sp_df.col2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conditional filtering</th>\n",
       "      <td>pd_df[pd_df.c1 &gt; 0]</td>\n",
       "      <td>sp_df.filter(df.c1 &gt; 0), sp_df.where(df.c1 &gt; 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conditional assigning</th>\n",
       "      <td>np.where(pd_df.c1.array &gt; 0, \"positive\")</td>\n",
       "      <td>sp_df.select(df.c1, when(df.c1 &gt; 0, \"positive\").alias(\"number_sign\"))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conditional assigning with else</th>\n",
       "      <td>np.where(pd_df.c1.array &gt; 0, \"pos\", \"neg\")</td>\n",
       "      <td>sp_df.select(df.c1, when(df.c1 &gt; 0, \"pos\").otherwise(\"neg\").alias(\"number_sign\"))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort 1 col asc</th>\n",
       "      <td>pd_df.sort_values(by=[\"c1\"])</td>\n",
       "      <td>sp_df.sort(sp_df.c1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort 2+ cols asc</th>\n",
       "      <td>pd_df.sort_values(by=[\"c1\",\"c2\"])</td>\n",
       "      <td>sp_df.sort(sp_df.c1, sp_df.c2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort 2+ cols desc/asc</th>\n",
       "      <td>pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=[False, True])</td>\n",
       "      <td>sp_df.sort(sp_df.c1.desc(), sp_df.c2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort 2+ cols desc</th>\n",
       "      <td>pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=False)</td>\n",
       "      <td>sp_df.sort(desc(\"c1\"), desc(\"c2\")) OR sp_df.sort(col(\"c1\").desc(), col(\"c2\").desc())</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     pandas  \\\n",
       "csv                                                               pd.read_csv(\"myfile.csv\")   \n",
       "json                                                            pd.read_json(\"myfile.json\")   \n",
       "1st n rows                                                                     pd_df.head()   \n",
       "1st row                                                                       pd_df.head(1)   \n",
       "summary statistics                                                         pd_df.describe()   \n",
       "column names                                                                  pd_df.columns   \n",
       "# rows                                                                           len(pd_df)   \n",
       "# distinct rows                                                len(pd_df.drop_duplicates())   \n",
       "df schema info                                                                 pd_df.info()   \n",
       "select columns                                                      pd_df[[\"col1\", \"col2\"]]   \n",
       "conditional filtering                                                   pd_df[pd_df.c1 > 0]   \n",
       "conditional assigning                              np.where(pd_df.c1.array > 0, \"positive\")   \n",
       "conditional assigning with else                  np.where(pd_df.c1.array > 0, \"pos\", \"neg\")   \n",
       "sort 1 col asc                                                 pd_df.sort_values(by=[\"c1\"])   \n",
       "sort 2+ cols asc                                          pd_df.sort_values(by=[\"c1\",\"c2\"])   \n",
       "sort 2+ cols desc/asc            pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=[False, True])   \n",
       "sort 2+ cols desc                        pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=False)   \n",
       "\n",
       "                                                                                                                spark  \n",
       "csv                                                          spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")  \n",
       "json                                spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")  \n",
       "1st n rows                                                                             sp_df.show(), .head(), .take()  \n",
       "1st row                                                                                                 sp_df.first()  \n",
       "summary statistics                                                                                   sp_df.describe()  \n",
       "column names                                                                                            sp_df.columns  \n",
       "# rows                                                                                                  sp_df.count()  \n",
       "# distinct rows                                                                              sp_df.distinct().count()  \n",
       "df schema info                                                                                    sp_df.printSchema()  \n",
       "select columns                                                                   sp_df.select(sp_df.col1, sp_df.col2)  \n",
       "conditional filtering                                                 sp_df.filter(df.c1 > 0), sp_df.where(df.c1 > 0)  \n",
       "conditional assigning                           sp_df.select(df.c1, when(df.c1 > 0, \"positive\").alias(\"number_sign\"))  \n",
       "conditional assigning with else     sp_df.select(df.c1, when(df.c1 > 0, \"pos\").otherwise(\"neg\").alias(\"number_sign\"))  \n",
       "sort 1 col asc                                                                                   sp_df.sort(sp_df.c1)  \n",
       "sort 2+ cols asc                                                                       sp_df.sort(sp_df.c1, sp_df.c2)  \n",
       "sort 2+ cols desc/asc                                                           sp_df.sort(sp_df.c1.desc(), sp_df.c2)  \n",
       "sort 2+ cols desc                sp_df.sort(desc(\"c1\"), desc(\"c2\")) OR sp_df.sort(col(\"c1\").desc(), col(\"c2\").desc())  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_v_spark = pd.DataFrame([['pd.read_csv(\"myfile.csv\")', \n",
    "                            'spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")'], \n",
    "                           ['pd.read_json(\"myfile.json\")', \n",
    "                            'spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")']], \n",
    "                          index = ['csv', 'json'], \n",
    "                          columns = ['pandas', 'spark'])\n",
    "\n",
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['pd_df.head()', 'sp_df.show(), .head(), .take()'],\n",
    "                                             ['pd_df.head(1)', 'sp_df.first()'],\n",
    "                                             ['pd_df.describe()', 'sp_df.describe()'],\n",
    "                                             ['pd_df.columns', 'sp_df.columns'],\n",
    "                                             ['len(pd_df)', 'sp_df.count()'],\n",
    "                                             ['len(pd_df.drop_duplicates())', 'sp_df.distinct().count()'],\n",
    "                                             ['pd_df.info()', 'sp_df.printSchema()']\n",
    "                                            ],\n",
    "                                            index = ['1st n rows', '1st row','summary statistics', \n",
    "                                                     'column names', '# rows', '# distinct rows', \n",
    "                                                     'df schema info'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "\n",
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['pd_df[[\"col1\", \"col2\"]]', \n",
    "                                              'sp_df.select(sp_df.col1, sp_df.col2)']\n",
    "                                            ],\n",
    "                                            index = ['select columns'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "\n",
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['pd_df[pd_df.c1 > 0]', 'sp_df.filter(df.c1 > 0), sp_df.where(df.c1 > 0)'],\n",
    "                                            ],\n",
    "                                            index = ['conditional filtering'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "\n",
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['np.where(pd_df.c1.array > 0, \"positive\")', \n",
    "                                              'sp_df.select(df.c1, when(df.c1 > 0, \"positive\").alias(\"number_sign\"))'],\n",
    "                                            ],\n",
    "                                            index = ['conditional assigning'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "\n",
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['np.where(pd_df.c1.array > 0, \"pos\", \"neg\")', \n",
    "                                              'sp_df.select(df.c1, when(df.c1 > 0, \"pos\").otherwise(\"neg\").alias(\"number_sign\"))'],\n",
    "                                            ],\n",
    "                                            index = ['conditional assigning with else'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "\n",
    "\n",
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['pd_df.sort_values(by=[\"c1\"])', \n",
    "                                              'sp_df.sort(sp_df.c1)'],\n",
    "                                             ['pd_df.sort_values(by=[\"c1\",\"c2\"])',\n",
    "                                              'sp_df.sort(sp_df.c1, sp_df.c2)'],\n",
    "                                             ['pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=[False, True])',\n",
    "                                              'sp_df.sort(sp_df.c1.desc(), sp_df.c2)'],\n",
    "                                             ['pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=False)', \n",
    "                                              'sp_df.sort(desc(\"c1\"), desc(\"c2\")) OR sp_df.sort(col(\"c1\").desc(), col(\"c2\").desc())']\n",
    "                                            ],\n",
    "                                            index = ['sort 1 col asc', 'sort 2+ cols asc', 'sort 2+ cols desc/asc', 'sort 2+ cols desc'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "pd_v_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1. Create a spark data frame that contains your favorite programming languages.\n",
    "\n",
    "### -The name of the column should be language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas dataframe by columns using dictionary-like object\n",
    "\n",
    "pd_df = pd.DataFrame({'col1': ['r1c1', 'r2c1', 'r3c1'],\n",
    "                      'col2': ['r1c2', 'r2c2', 'r3c2'],\n",
    "                      'col3': ['r1c3', 'r3c3', 'r3c3']\n",
    "                        }, \n",
    "                     index = [1, 2, 3])\n",
    "pd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -View the schema of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -Output the shape of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -Show the first 5 records in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2. Load the mpg dataset as a spark dataframe.\n",
    "\n",
    "### - A. Create 1 column of output that contains a message like the one below:\n",
    "   `The 1999 audi a4 has a 4 cylinder engine.`\n",
    "\n",
    "### - B. For each vehicle.\n",
    "        - Transform the trans column so that it only contains either manual or auto.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3. Load the tips dataset as a spark dataframe.\n",
    "\n",
    "### - A. What percentage of observations are smokers?\n",
    "### - B. Create a column that contains the tip percentage\n",
    "### - C. Calculate the average tip percentage for each combination of sex and smoker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4. Use the seattle weather dataset referenced in the lesson to answer the questions below.\n",
    "\n",
    "### -Convert the temperatures to farenheight.\n",
    "### -Which month has the most rain, on average?\n",
    "### -Which year was the windiest?\n",
    "### -What is the most frequent type of weather in January?\n",
    "### -What is the average high and low tempurature on sunny days in July in 2013 and 2014?\n",
    "### -What percentage of days were rainy in q3 of 2015?\n",
    "### -For each year, find what percentage of days it rained (had non-zero precipitation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
