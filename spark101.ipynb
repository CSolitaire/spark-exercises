{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark 101 Exercises\n",
    "### Corey Solitaire\n",
    "`11.24.2020`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pydataset import data\n",
    "from vega_datasets import data\n",
    "\n",
    "from pyspark.sql.functions import col, expr, concat, sum, avg, min, max, count, mean, round\n",
    "from pyspark.sql.functions import lit, regexp_extract, regexp_replace, when,asc, desc, month, year, quarter  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Method to create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Reference Material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pandas</th>\n",
       "      <th>spark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>csv</th>\n",
       "      <td>pd.read_csv(\"myfile.csv\")</td>\n",
       "      <td>spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>json</th>\n",
       "      <td>pd.read_json(\"myfile.json\")</td>\n",
       "      <td>spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st n rows</th>\n",
       "      <td>pd_df.head()</td>\n",
       "      <td>sp_df.show(), .head(), .take()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st row</th>\n",
       "      <td>pd_df.head(1)</td>\n",
       "      <td>sp_df.first()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary statistics</th>\n",
       "      <td>pd_df.describe()</td>\n",
       "      <td>sp_df.describe()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column names</th>\n",
       "      <td>pd_df.columns</td>\n",
       "      <td>sp_df.columns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># rows</th>\n",
       "      <td>len(pd_df)</td>\n",
       "      <td>sp_df.count()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># distinct rows</th>\n",
       "      <td>len(pd_df.drop_duplicates())</td>\n",
       "      <td>sp_df.distinct().count()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>df schema info</th>\n",
       "      <td>pd_df.info()</td>\n",
       "      <td>sp_df.printSchema()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select columns</th>\n",
       "      <td>pd_df[[\"col1\", \"col2\"]]</td>\n",
       "      <td>sp_df.select(sp_df.col1, sp_df.col2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conditional filtering</th>\n",
       "      <td>pd_df[pd_df.c1 &gt; 0]</td>\n",
       "      <td>sp_df.filter(df.c1 &gt; 0), sp_df.where(df.c1 &gt; 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conditional assigning</th>\n",
       "      <td>np.where(pd_df.c1.array &gt; 0, \"positive\")</td>\n",
       "      <td>sp_df.select(df.c1, when(df.c1 &gt; 0, \"positive\").alias(\"number_sign\"))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conditional assigning with else</th>\n",
       "      <td>np.where(pd_df.c1.array &gt; 0, \"pos\", \"neg\")</td>\n",
       "      <td>sp_df.select(df.c1, when(df.c1 &gt; 0, \"pos\").otherwise(\"neg\").alias(\"number_sign\"))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort 1 col asc</th>\n",
       "      <td>pd_df.sort_values(by=[\"c1\"])</td>\n",
       "      <td>sp_df.sort(sp_df.c1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort 2+ cols asc</th>\n",
       "      <td>pd_df.sort_values(by=[\"c1\",\"c2\"])</td>\n",
       "      <td>sp_df.sort(sp_df.c1, sp_df.c2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort 2+ cols desc/asc</th>\n",
       "      <td>pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=[False, True])</td>\n",
       "      <td>sp_df.sort(sp_df.c1.desc(), sp_df.c2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort 2+ cols desc</th>\n",
       "      <td>pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=False)</td>\n",
       "      <td>sp_df.sort(desc(\"c1\"), desc(\"c2\")) OR sp_df.sort(col(\"c1\").desc(), col(\"c2\").desc())</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     pandas  \\\n",
       "csv                                                               pd.read_csv(\"myfile.csv\")   \n",
       "json                                                            pd.read_json(\"myfile.json\")   \n",
       "1st n rows                                                                     pd_df.head()   \n",
       "1st row                                                                       pd_df.head(1)   \n",
       "summary statistics                                                         pd_df.describe()   \n",
       "column names                                                                  pd_df.columns   \n",
       "# rows                                                                           len(pd_df)   \n",
       "# distinct rows                                                len(pd_df.drop_duplicates())   \n",
       "df schema info                                                                 pd_df.info()   \n",
       "select columns                                                      pd_df[[\"col1\", \"col2\"]]   \n",
       "conditional filtering                                                   pd_df[pd_df.c1 > 0]   \n",
       "conditional assigning                              np.where(pd_df.c1.array > 0, \"positive\")   \n",
       "conditional assigning with else                  np.where(pd_df.c1.array > 0, \"pos\", \"neg\")   \n",
       "sort 1 col asc                                                 pd_df.sort_values(by=[\"c1\"])   \n",
       "sort 2+ cols asc                                          pd_df.sort_values(by=[\"c1\",\"c2\"])   \n",
       "sort 2+ cols desc/asc            pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=[False, True])   \n",
       "sort 2+ cols desc                        pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=False)   \n",
       "\n",
       "                                                                                                                spark  \n",
       "csv                                                          spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")  \n",
       "json                                spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")  \n",
       "1st n rows                                                                             sp_df.show(), .head(), .take()  \n",
       "1st row                                                                                                 sp_df.first()  \n",
       "summary statistics                                                                                   sp_df.describe()  \n",
       "column names                                                                                            sp_df.columns  \n",
       "# rows                                                                                                  sp_df.count()  \n",
       "# distinct rows                                                                              sp_df.distinct().count()  \n",
       "df schema info                                                                                    sp_df.printSchema()  \n",
       "select columns                                                                   sp_df.select(sp_df.col1, sp_df.col2)  \n",
       "conditional filtering                                                 sp_df.filter(df.c1 > 0), sp_df.where(df.c1 > 0)  \n",
       "conditional assigning                           sp_df.select(df.c1, when(df.c1 > 0, \"positive\").alias(\"number_sign\"))  \n",
       "conditional assigning with else     sp_df.select(df.c1, when(df.c1 > 0, \"pos\").otherwise(\"neg\").alias(\"number_sign\"))  \n",
       "sort 1 col asc                                                                                   sp_df.sort(sp_df.c1)  \n",
       "sort 2+ cols asc                                                                       sp_df.sort(sp_df.c1, sp_df.c2)  \n",
       "sort 2+ cols desc/asc                                                           sp_df.sort(sp_df.c1.desc(), sp_df.c2)  \n",
       "sort 2+ cols desc                sp_df.sort(desc(\"c1\"), desc(\"c2\")) OR sp_df.sort(col(\"c1\").desc(), col(\"c2\").desc())  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_v_spark = pd.DataFrame([['pd.read_csv(\"myfile.csv\")', \n",
    "                            'spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")'], \n",
    "                           ['pd.read_json(\"myfile.json\")', \n",
    "                            'spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")']], \n",
    "                          index = ['csv', 'json'], \n",
    "                          columns = ['pandas', 'spark'])\n",
    "\n",
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['pd_df.head()', 'sp_df.show(), .head(), .take()'],\n",
    "                                             ['pd_df.head(1)', 'sp_df.first()'],\n",
    "                                             ['pd_df.describe()', 'sp_df.describe()'],\n",
    "                                             ['pd_df.columns', 'sp_df.columns'],\n",
    "                                             ['len(pd_df)', 'sp_df.count()'],\n",
    "                                             ['len(pd_df.drop_duplicates())', 'sp_df.distinct().count()'],\n",
    "                                             ['pd_df.info()', 'sp_df.printSchema()']\n",
    "                                            ],\n",
    "                                            index = ['1st n rows', '1st row','summary statistics', \n",
    "                                                     'column names', '# rows', '# distinct rows', \n",
    "                                                     'df schema info'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "\n",
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['pd_df[[\"col1\", \"col2\"]]', \n",
    "                                              'sp_df.select(sp_df.col1, sp_df.col2)']\n",
    "                                            ],\n",
    "                                            index = ['select columns'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "\n",
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['pd_df[pd_df.c1 > 0]', 'sp_df.filter(df.c1 > 0), sp_df.where(df.c1 > 0)'],\n",
    "                                            ],\n",
    "                                            index = ['conditional filtering'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "\n",
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['np.where(pd_df.c1.array > 0, \"positive\")', \n",
    "                                              'sp_df.select(df.c1, when(df.c1 > 0, \"positive\").alias(\"number_sign\"))'],\n",
    "                                            ],\n",
    "                                            index = ['conditional assigning'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "\n",
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['np.where(pd_df.c1.array > 0, \"pos\", \"neg\")', \n",
    "                                              'sp_df.select(df.c1, when(df.c1 > 0, \"pos\").otherwise(\"neg\").alias(\"number_sign\"))'],\n",
    "                                            ],\n",
    "                                            index = ['conditional assigning with else'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "\n",
    "\n",
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['pd_df.sort_values(by=[\"c1\"])', \n",
    "                                              'sp_df.sort(sp_df.c1)'],\n",
    "                                             ['pd_df.sort_values(by=[\"c1\",\"c2\"])',\n",
    "                                              'sp_df.sort(sp_df.c1, sp_df.c2)'],\n",
    "                                             ['pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=[False, True])',\n",
    "                                              'sp_df.sort(sp_df.c1.desc(), sp_df.c2)'],\n",
    "                                             ['pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=False)', \n",
    "                                              'sp_df.sort(desc(\"c1\"), desc(\"c2\")) OR sp_df.sort(col(\"c1\").desc(), col(\"c2\").desc())']\n",
    "                                            ],\n",
    "                                            index = ['sort 1 col asc', 'sort 2+ cols asc', 'sort 2+ cols desc/asc', 'sort 2+ cols desc'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "pd_v_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1. Create a spark data frame that contains your favorite programming languages.\n",
    "\n",
    "### -The name of the column should be language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>python</th>\n",
       "      <th>javascript</th>\n",
       "      <th>r</th>\n",
       "      <th>java</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>csv</th>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>json</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     python javascript    r java\n",
       "csv     yes         no  yes   no\n",
       "json    yes        yes  yes  yes"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Dataframe\n",
    "lang_df = pd.DataFrame([['yes', 'no', 'yes', 'no'], \n",
    "                        ['yes', 'yes', 'yes', 'yes']], \n",
    "                        index = ['csv', 'json'], \n",
    "                        columns = ['python', 'javascript', 'r', 'java'])\n",
    "\n",
    "# to display and see all text in dataframe\n",
    "pd.set_option('display.max_colwidth', 10000)\n",
    "\n",
    "\n",
    "lang_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+----+\n",
      "|python|javascript|  r|java|\n",
      "+------+----------+---+----+\n",
      "|   yes|        no|yes|  no|\n",
      "|   yes|       yes|yes| yes|\n",
      "+------+----------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to spark\n",
    "sp_df = spark.createDataFrame(lang_df)\n",
    "sp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -View the schema of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- python: string (nullable = true)\n",
      " |-- javascript: string (nullable = true)\n",
      " |-- r: string (nullable = true)\n",
      " |-- java: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -Output the shape of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4)\n"
     ]
    }
   ],
   "source": [
    "print((sp_df.count(), len(sp_df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -Show the first 5 records in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+----+\n",
      "|python|javascript|  r|java|\n",
      "+------+----------+---+----+\n",
      "|   yes|        no|yes|  no|\n",
      "|   yes|       yes|yes| yes|\n",
      "+------+----------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2. Load the mpg dataset as a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>model</th>\n",
       "      <th>displ</th>\n",
       "      <th>year</th>\n",
       "      <th>cyl</th>\n",
       "      <th>trans</th>\n",
       "      <th>drv</th>\n",
       "      <th>cty</th>\n",
       "      <th>hwy</th>\n",
       "      <th>fl</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>audi</td>\n",
       "      <td>a4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1999</td>\n",
       "      <td>4</td>\n",
       "      <td>auto(l5)</td>\n",
       "      <td>f</td>\n",
       "      <td>18</td>\n",
       "      <td>29</td>\n",
       "      <td>p</td>\n",
       "      <td>compact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>audi</td>\n",
       "      <td>a4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1999</td>\n",
       "      <td>4</td>\n",
       "      <td>manual(m5)</td>\n",
       "      <td>f</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>p</td>\n",
       "      <td>compact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>audi</td>\n",
       "      <td>a4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>manual(m6)</td>\n",
       "      <td>f</td>\n",
       "      <td>20</td>\n",
       "      <td>31</td>\n",
       "      <td>p</td>\n",
       "      <td>compact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>audi</td>\n",
       "      <td>a4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>auto(av)</td>\n",
       "      <td>f</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>p</td>\n",
       "      <td>compact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>audi</td>\n",
       "      <td>a4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1999</td>\n",
       "      <td>6</td>\n",
       "      <td>auto(l5)</td>\n",
       "      <td>f</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "      <td>p</td>\n",
       "      <td>compact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  manufacturer model  displ  year  cyl       trans drv  cty  hwy fl    class\n",
       "1         audi    a4    1.8  1999    4    auto(l5)   f   18   29  p  compact\n",
       "2         audi    a4    1.8  1999    4  manual(m5)   f   21   29  p  compact\n",
       "3         audi    a4    2.0  2008    4  manual(m6)   f   20   31  p  compact\n",
       "4         audi    a4    2.0  2008    4    auto(av)   f   21   30  p  compact\n",
       "5         audi    a4    2.8  1999    6    auto(l5)   f   16   26  p  compact"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydataset import data\n",
    "\n",
    "mpg_pd = data(\"mpg\")\n",
    "mpg_pd.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - A. Create 1 column of output that contains a message like the one below:\n",
    "   `The 1999 audi a4 has a 4 cylinder engine.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - B. For each vehicle.\n",
    "        - Transform the trans column so that it only contains either manual or auto.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3. Load the tips dataset as a spark dataframe.\n",
    "\n",
    "### - A. What percentage of observations are smokers?\n",
    "### - B. Create a column that contains the tip percentage\n",
    "### - C. Calculate the average tip percentage for each combination of sex and smoker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4. Use the seattle weather dataset referenced in the lesson to answer the questions below.\n",
    "\n",
    "### -Convert the temperatures to farenheight.\n",
    "### -Which month has the most rain, on average?\n",
    "### -Which year was the windiest?\n",
    "### -What is the most frequent type of weather in January?\n",
    "### -What is the average high and low tempurature on sunny days in July in 2013 and 2014?\n",
    "### -What percentage of days were rainy in q3 of 2015?\n",
    "### -For each year, find what percentage of days it rained (had non-zero precipitation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
